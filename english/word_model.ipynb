{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import codecs\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = pd.read_csv('C:/Users/jw156/Ironhack/vaccine/newspapers.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['hrefs', 'title', 'new_title', 'id', 'date', 'newspaper_name',\n",
       "       'vaccine', 'pos_headline', 'lem_pos_headline'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspapers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = newspapers.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Determination of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers = newspapers.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vaccine\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def pos_headline(headline):\n",
    "    new_headline = headline.split(' ')\n",
    "    word_list = []\n",
    "    for word in new_headline:\n",
    "        if word == None or word.isalpha()==False:\n",
    "            continue\n",
    "        else:\n",
    "            text = word_tokenize(word)\n",
    "            postition_tag = nltk.pos_tag(text)\n",
    "            word_list.append(postition_tag)\n",
    "    return word_list\n",
    "\n",
    "def is_proper_noun(tag):\n",
    "    return tag in ['NNP', 'NNPS']\n",
    "\n",
    "def is_noun(tag):\n",
    "    return tag in ['NN', 'NNS']\n",
    "\n",
    "\n",
    "def is_verb(tag):\n",
    "    return tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "\n",
    "def is_adverb(tag):\n",
    "    return tag in ['RB', 'RBR', 'RBS']\n",
    "\n",
    "\n",
    "def is_adjective(tag):\n",
    "    return tag in ['JJ', 'JJR', 'JJS']\n",
    "\n",
    "\n",
    "\n",
    "def penn_to_wn(headlines):\n",
    "    headline_list = pos_headline(headlines)\n",
    "    tagged_list = []\n",
    "    for tag in headline_list:\n",
    "        tag = list(tag[0])\n",
    "        if is_adjective(tag[1]):\n",
    "            tag[1] = (wn.ADJ)\n",
    "            tagged_list.append(tag)\n",
    "        elif is_noun(tag[1]):\n",
    "            tag[1] =(wn.NOUN)\n",
    "            tagged_list.append(tag)\n",
    "        elif is_adverb(tag[1]):\n",
    "            tag[1] =(wn.ADV)\n",
    "            tagged_list.append(tag)\n",
    "        elif is_verb(tag[1]):\n",
    "            tag[1] =(wn.VERB)\n",
    "            tagged_list.append(tag)\n",
    "        elif is_proper_noun(tag[1]):\n",
    "            tag[1] =('NNP')\n",
    "            tagged_list.append(tag)\n",
    "    return tagged_list\n",
    "\n",
    "def lemming_words(headline):\n",
    "    if len(headline) == 0:\n",
    "        return 0\n",
    "    headline_list = penn_to_wn(headline)    \n",
    "    new_string = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word in headline_list:\n",
    "        if word[1] == 'NNP':\n",
    "            continue\n",
    "        else:\n",
    "            word[0] = (lemmatizer.lemmatize(word[0], word[1]))\n",
    "            new_string.append(word)\n",
    "\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Scepticism', 'n'],\n",
       " ['Oxford', 'n'],\n",
       " ['vaccine', 'n'],\n",
       " ['threatens', 'n'],\n",
       " ['immunisation', 'n'],\n",
       " ['push', 'n']]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = newspapers.title[1]\n",
    "lemming_words(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers['pos_headline'] = newspapers['title'].apply(pos_headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers['lem_pos_headline'] = newspapers['title'].apply(lambda x: lemming_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Scepticism', 'NN')],\n",
       " [('over', 'IN')],\n",
       " [('Oxford', 'NN')],\n",
       " [('vaccine', 'NN')],\n",
       " [('threatens', 'NNS')],\n",
       " [('Europe', 'NNP'), (\"'s\", 'POS')],\n",
       " [('immunisation', 'NN')],\n",
       " [('push', 'NN')]]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspapers['pos_headline'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('threaten.v.03')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "x = 'The porn threatens to cause a lockdown'\n",
    "\n",
    "print(lesk(x.split(), 'threatens', 'v'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_definitions(title, lemmed_title):\n",
    "    synsets = []\n",
    "    original_text = title\n",
    "    lemmed_pos = lemmed_title\n",
    "    for i in lemmed_pos:\n",
    "        synsets.append(lesk(original_text.split(), i[0], i[1]))\n",
    "    return synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newspapers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5fd04580d2a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewspapers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewspapers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lem_pos_headline'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_definitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'newspapers' is not defined"
     ]
    }
   ],
   "source": [
    "x = newspapers['title'][1]\n",
    "y = newspapers['lem_pos_headline'][1]\n",
    "import re\n",
    "\n",
    "j = word_definitions(x, y)\n",
    "k = str(j[0])\n",
    "print(k)\n",
    "x = (re.findall(r\"'([^']*)'\", k))\n",
    "f = swn.senti_synset(x[0])\n",
    "# print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "newspapers['word_definitions'] = newspapers.apply(lambda x: word_definitions(x['title'], x['lem_pos_headline']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hrefs</th>\n",
       "      <th>title</th>\n",
       "      <th>new_title</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>newspaper_name</th>\n",
       "      <th>vaccine</th>\n",
       "      <th>pos_headline</th>\n",
       "      <th>lem_pos_headline</th>\n",
       "      <th>word_definitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.theguardian.com/politics/2021/mar/...</td>\n",
       "      <td>Boris Johnson receives Oxford/AstraZeneca Covi...</td>\n",
       "      <td>bor johnson receiv oxfordastrazenec covid vaccin</td>\n",
       "      <td>politics</td>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>guardian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[(Boris, NNP)], [(Johnson, NNP)], [(receives,...</td>\n",
       "      <td>[[receives, n], [Covid, n], [vaccine, n]]</td>\n",
       "      <td>[None, None, Synset('vaccine.n.01')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.theguardian.com/world/2021/feb/19/...</td>\n",
       "      <td>Scepticism over Oxford vaccine threatens Europ...</td>\n",
       "      <td>sceiv oxford vaccin threatens europ immun push</td>\n",
       "      <td>world</td>\n",
       "      <td>2021-02-19</td>\n",
       "      <td>guardian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[(Scepticism, NN)], [(over, IN)], [(Oxford, N...</td>\n",
       "      <td>[[Scepticism, n], [Oxford, n], [vaccine, n], [...</td>\n",
       "      <td>[Synset('agnosticism.n.02'), Synset('oxford.n....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.theguardian.com/world/2021/mar/18/...</td>\n",
       "      <td>Thursday briefing: EU's experts to give Oxford...</td>\n",
       "      <td>thursday brief eu expert giv oxford vaccin ver...</td>\n",
       "      <td>world</td>\n",
       "      <td>2021-03-18</td>\n",
       "      <td>guardian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[(Thursday, NNP)], [(briefing, NN), (:, :)], ...</td>\n",
       "      <td>[[expert, n], [give, v], [Oxford, n], [vaccine...</td>\n",
       "      <td>[Synset('expert.n.01'), Synset('give.v.18'), S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.theguardian.com/world/2021/mar/16/...</td>\n",
       "      <td>Oxford/AstraZeneca vaccine: which countries ha...</td>\n",
       "      <td>oxfordastrazenec vaccin country paus jab</td>\n",
       "      <td>world</td>\n",
       "      <td>2021-03-16</td>\n",
       "      <td>guardian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[(Oxford/AstraZeneca, NN)], [(vaccine, NN), (...</td>\n",
       "      <td>[[country, n], [have, v], [pause, v], [jab, n]]</td>\n",
       "      <td>[Synset('country.n.04'), Synset('take.v.35'), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.theguardian.com/world/2021/mar/16/...</td>\n",
       "      <td>Chaos in Germany and Italy after suspension of...</td>\n",
       "      <td>chao germany ita suspend oxford vaccin</td>\n",
       "      <td>world</td>\n",
       "      <td>2021-03-16</td>\n",
       "      <td>guardian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[(Chaos, NN)], [(in, IN)], [(Germany, NNP)], ...</td>\n",
       "      <td>[[Chaos, n], [suspension, n], [Oxford, n], [va...</td>\n",
       "      <td>[Synset('chaos.n.02'), Synset('suspension.n.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62019</th>\n",
       "      <td>https://www.independent.co.uk/news/uk/politics...</td>\n",
       "      <td>Extreme lockdown laws extended for a further s...</td>\n",
       "      <td>extrem lockdown law extend six month despit ma...</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>independent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[(Extreme, NN)], [(lockdown, NN)], [(laws, NN...</td>\n",
       "      <td>[[Extreme, n], [lockdown, n], [law, n], [exten...</td>\n",
       "      <td>[Synset('extreme_point.n.01'), Synset('lockdow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62020</th>\n",
       "      <td>https://www.independent.co.uk/news/uk/politics...</td>\n",
       "      <td>Extreme lockdown laws extended for a further s...</td>\n",
       "      <td>extrem lockdown law extend six month despit ma...</td>\n",
       "      <td>coronavirus</td>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>independent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[(Extreme, NN)], [(lockdown, NN)], [(laws, NN...</td>\n",
       "      <td>[[Extreme, n], [lockdown, n], [law, n], [exten...</td>\n",
       "      <td>[Synset('extreme_point.n.01'), Synset('lockdow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62021</th>\n",
       "      <td>https://www.independent.co.uk/independentpremi...</td>\n",
       "      <td>The problem with Johnson’s vaccine passports f...</td>\n",
       "      <td>the problem johnson vaccin passport pub</td>\n",
       "      <td>business</td>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>independent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[(The, DT)], [(problem, NN)], [(with, IN)], [...</td>\n",
       "      <td>[[problem, n], [vaccine, n], [passport, n], [p...</td>\n",
       "      <td>[Synset('problem.n.02'), Synset('vaccine.n.01'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62022</th>\n",
       "      <td>https://www.independent.co.uk/voices/editorial...</td>\n",
       "      <td>Reality is dawning on the EU: export bans are ...</td>\n",
       "      <td>real dawn eu export ban counterproduc cas covi...</td>\n",
       "      <td>uk</td>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>independent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[(Reality, NN)], [(is, VBZ)], [(dawning, VBG)...</td>\n",
       "      <td>[[Reality, n], [be, v], [dawn, v], [export, n]...</td>\n",
       "      <td>[Synset('reality.n.03'), Synset('embody.v.02')...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62023</th>\n",
       "      <td>https://www.independent.co.uk/news/uk/home-new...</td>\n",
       "      <td>University of Aberdeen to become one of the fi...</td>\n",
       "      <td>univers aberdeen becom on first uk institut re...</td>\n",
       "      <td>uk</td>\n",
       "      <td>2021-03-25</td>\n",
       "      <td>independent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[(University, NNP)], [(of, IN)], [(Aberdeen, ...</td>\n",
       "      <td>[[Aberdeen, n], [become, n], [first, r], [UK, ...</td>\n",
       "      <td>[Synset('aberdeen.n.04'), None, Synset('first....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62024 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   hrefs  \\\n",
       "0      https://www.theguardian.com/politics/2021/mar/...   \n",
       "1      https://www.theguardian.com/world/2021/feb/19/...   \n",
       "2      https://www.theguardian.com/world/2021/mar/18/...   \n",
       "3      https://www.theguardian.com/world/2021/mar/16/...   \n",
       "4      https://www.theguardian.com/world/2021/mar/16/...   \n",
       "...                                                  ...   \n",
       "62019  https://www.independent.co.uk/news/uk/politics...   \n",
       "62020  https://www.independent.co.uk/news/uk/politics...   \n",
       "62021  https://www.independent.co.uk/independentpremi...   \n",
       "62022  https://www.independent.co.uk/voices/editorial...   \n",
       "62023  https://www.independent.co.uk/news/uk/home-new...   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Boris Johnson receives Oxford/AstraZeneca Covi...   \n",
       "1      Scepticism over Oxford vaccine threatens Europ...   \n",
       "2      Thursday briefing: EU's experts to give Oxford...   \n",
       "3      Oxford/AstraZeneca vaccine: which countries ha...   \n",
       "4      Chaos in Germany and Italy after suspension of...   \n",
       "...                                                  ...   \n",
       "62019  Extreme lockdown laws extended for a further s...   \n",
       "62020  Extreme lockdown laws extended for a further s...   \n",
       "62021  The problem with Johnson’s vaccine passports f...   \n",
       "62022  Reality is dawning on the EU: export bans are ...   \n",
       "62023  University of Aberdeen to become one of the fi...   \n",
       "\n",
       "                                               new_title           id  \\\n",
       "0      bor johnson receiv oxfordastrazenec covid vaccin      politics   \n",
       "1        sceiv oxford vaccin threatens europ immun push         world   \n",
       "2      thursday brief eu expert giv oxford vaccin ver...        world   \n",
       "3              oxfordastrazenec vaccin country paus jab         world   \n",
       "4                chao germany ita suspend oxford vaccin         world   \n",
       "...                                                  ...          ...   \n",
       "62019  extrem lockdown law extend six month despit ma...  coronavirus   \n",
       "62020  extrem lockdown law extend six month despit ma...  coronavirus   \n",
       "62021           the problem johnson vaccin passport pub      business   \n",
       "62022  real dawn eu export ban counterproduc cas covi...           uk   \n",
       "62023  univers aberdeen becom on first uk institut re...           uk   \n",
       "\n",
       "             date newspaper_name vaccine  \\\n",
       "0      2021-03-19       guardian     NaN   \n",
       "1      2021-02-19       guardian     NaN   \n",
       "2      2021-03-18       guardian     NaN   \n",
       "3      2021-03-16       guardian     NaN   \n",
       "4      2021-03-16       guardian     NaN   \n",
       "...           ...            ...     ...   \n",
       "62019  2021-03-25    independent     NaN   \n",
       "62020  2021-03-25    independent     NaN   \n",
       "62021  2021-03-25    independent     NaN   \n",
       "62022  2021-03-25    independent     NaN   \n",
       "62023  2021-03-25    independent     NaN   \n",
       "\n",
       "                                            pos_headline  \\\n",
       "0      [[(Boris, NNP)], [(Johnson, NNP)], [(receives,...   \n",
       "1      [[(Scepticism, NN)], [(over, IN)], [(Oxford, N...   \n",
       "2      [[(Thursday, NNP)], [(briefing, NN), (:, :)], ...   \n",
       "3      [[(Oxford/AstraZeneca, NN)], [(vaccine, NN), (...   \n",
       "4      [[(Chaos, NN)], [(in, IN)], [(Germany, NNP)], ...   \n",
       "...                                                  ...   \n",
       "62019  [[(Extreme, NN)], [(lockdown, NN)], [(laws, NN...   \n",
       "62020  [[(Extreme, NN)], [(lockdown, NN)], [(laws, NN...   \n",
       "62021  [[(The, DT)], [(problem, NN)], [(with, IN)], [...   \n",
       "62022  [[(Reality, NN)], [(is, VBZ)], [(dawning, VBG)...   \n",
       "62023  [[(University, NNP)], [(of, IN)], [(Aberdeen, ...   \n",
       "\n",
       "                                        lem_pos_headline  \\\n",
       "0              [[receives, n], [Covid, n], [vaccine, n]]   \n",
       "1      [[Scepticism, n], [Oxford, n], [vaccine, n], [...   \n",
       "2      [[expert, n], [give, v], [Oxford, n], [vaccine...   \n",
       "3        [[country, n], [have, v], [pause, v], [jab, n]]   \n",
       "4      [[Chaos, n], [suspension, n], [Oxford, n], [va...   \n",
       "...                                                  ...   \n",
       "62019  [[Extreme, n], [lockdown, n], [law, n], [exten...   \n",
       "62020  [[Extreme, n], [lockdown, n], [law, n], [exten...   \n",
       "62021  [[problem, n], [vaccine, n], [passport, n], [p...   \n",
       "62022  [[Reality, n], [be, v], [dawn, v], [export, n]...   \n",
       "62023  [[Aberdeen, n], [become, n], [first, r], [UK, ...   \n",
       "\n",
       "                                        word_definitions  \n",
       "0                   [None, None, Synset('vaccine.n.01')]  \n",
       "1      [Synset('agnosticism.n.02'), Synset('oxford.n....  \n",
       "2      [Synset('expert.n.01'), Synset('give.v.18'), S...  \n",
       "3      [Synset('country.n.04'), Synset('take.v.35'), ...  \n",
       "4      [Synset('chaos.n.02'), Synset('suspension.n.05...  \n",
       "...                                                  ...  \n",
       "62019  [Synset('extreme_point.n.01'), Synset('lockdow...  \n",
       "62020  [Synset('extreme_point.n.01'), Synset('lockdow...  \n",
       "62021  [Synset('problem.n.02'), Synset('vaccine.n.01'...  \n",
       "62022  [Synset('reality.n.03'), Synset('embody.v.02')...  \n",
       "62023  [Synset('aberdeen.n.04'), None, Synset('first....  \n",
       "\n",
       "[62024 rows x 10 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspapers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
